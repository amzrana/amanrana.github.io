# Published work on Mixture of Experts (MoE)

> **Date published:** 10th Dec, 2023

### [HOME](../index.md)

&nbsp;

## Published work
1. Adaptive Mixture of Local Experts ([arXiv](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf))
1. Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer ([arXiv](https://arxiv.org/abs/1701.06538))
1. Cross-token Modeling with Conditional Computation ([arXiv](https://arxiv.org/abs/2109.02008))
1. MoEfication: Transformer Feed-forward Layers are Mixtures of Experts ([arXiv](https://arxiv.org/abs/2110.01786))
1. Towards More Effective and Economic Sparsely-Activated Model ([arXiv](https://arxiv.org/abs/2110.07431))
1. Sparse MoEs meet Efficient Ensembles ([arXiv](https://arxiv.org/abs/2110.03360))
1. EvoMoE: An Evolutional Mixture-of-Experts Training Framework via Dense-To-Sparse Gate ([arXiv](https://arxiv.org/abs/2112.14397))
1. Unbiased Gradient Estimation with Balanced Assignments for Mixtures of Experts ([arXiv](https://arxiv.org/abs/2109.11817))
1. DSelect-k: Differentiable Selection in the Mixture of Experts with Applications to Multi-Task Learning ([arXiv](https://arxiv.org/abs/2106.03760))
1. Efficient Large Scale Language Modeling with Mixtures of Experts ([arXiv](https://arxiv.org/abs/2112.10684))
1. GLaM: Efficient Scaling of Language Models with Mixture-of-Experts ([arXiv](https://arxiv.org/abs/2112.06905))
1. Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity ([arXiv](https://arxiv.org/abs/2101.03961))
1. On the Representation Collapse of Sparse Mixture of Experts ([arXiv](https://arxiv.org/abs/2204.09179))
1. MoEC: Mixture of Expert Clusters ([arXiv](https://arxiv.org/abs/2207.09094))
1. Parameter-Efficient Mixture-of-Experts Architecture for Pre-trained Language Models ([arXiv](https://arxiv.org/abs/2203.01104))
1. Mixture-of-Experts with Expert Choice Routing ([arXiv](https://arxiv.org/abs/2202.09368))
1. Scaling Vision with Sparse Mixture of Experts ([arXiv](https://arxiv.org/abs/2106.05974))
1. StableMoE: Stable Routing Strategy for Mixture of Experts ([arXiv](https://arxiv.org/abs/2204.08396)) ([code](https://github.com/Hunter-DDM/stablemoe))
1. Go Wider Instead of Deeper ([arXiv](https://arxiv.org/abs/2107.11817))
1. Towards Understanding Mixture of Experts in Deep Learning ([arXiv](https://arxiv.org/abs/2208.02813))
1. Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models ([arXiv](https://arxiv.org/abs/2305.14705))
1. From Sparse to Soft Mixtures of Experts ([arXiv](https://arxiv.org/abs/2308.00951v1))
1. Language-Routing Mixture of Experts for Multilingual and Code-Switching Speech Recognition ([arXiv](https://arxiv.org/abs/2307.05956))
1. Mixture of Informed Experts for Multilingual Speech Recognition ([arXiv](https://ieeexplore.ieee.org/document/9414379)) **[Note: Behind IEEE paywall]**
1. Mixture-of-Expert Conformer for Streaming Multilingual ASR ([arXiv](https://arxiv.org/abs/2305.15663))
1. Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference ([arXiv](https://aclanthology.org/2021.findings-emnlp.304.pdf))
